# üìö Notable-LLM-Research-Papers
A curated collection of recent research papers related to Large Language Models üöÄ. Each entry includes a clickable link to the full paper. üåü

---

| üî¢ **S.No.** | üìù **Paper Title**                                                                                                      | üîó **Link**                         |
|--------------|-----------------------------------------------------------------------------------------------------------------------|-------------------------------------|
| 1            | LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning                                                      | [Link](https://arxiv.org/abs/2401.01325) |
| 2            | Knowledge Fusion of Large Language Models                                                                            | [Link](https://arxiv.org/abs/2401.10491) |
| 3            | A Comprehensive Study of Knowledge Editing for Large Language Models                                                 | [Link](https://arxiv.org/abs/2401.01286) |
| 4            | DiffusionGPT: LLM-Driven Text-to-Image Generation System                                                             | [Link](https://arxiv.org/abs/2401.10061) |
| 5            | Tuning Language Models by Proxy                                                                                      | [Link](https://arxiv.org/abs/2401.08565) |
| 6            | An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models                  | [Link](https://arxiv.org/abs/2401.06692) |
| 7            | Soaring from 4K to 400K: Extending LLM‚Äôs Context with Activation Beacon                                              | [Link](https://arxiv.org/abs/2401.03462) |
| 8            | VMamba: Visual State Space Model                                                                                     | [Link](https://arxiv.org/abs/2401.10166) |
| 9            | LLaMA Beyond English: An Empirical Study on Language Capability Transfer                                             | [Link](https://arxiv.org/abs/2401.01055) |
| 10           | DeepSeek LLM: Scaling Open-Source Language Models with Longtermism                                                   | [Link](https://arxiv.org/abs/2401.02954) |
| 11           | Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM                                     | [Link](https://arxiv.org/abs/2401.02994) |
| 12           | LLaMA Pro: Progressive LLaMA with Block Expansion                                                                    | [Link](https://arxiv.org/abs/2401.02415) |
| 13           | RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation                                                 | [Link](https://arxiv.org/abs/2401.04679) |
| 14           | Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text                                         | [Link](https://arxiv.org/abs/2401.12070) |
| 15           | Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling                                        | [Link](https://arxiv.org/abs/2401.16380) |
| 16           | WARM: On the Benefits of Weight Averaged Reward Models                                                               | [Link](https://arxiv.org/abs/2401.12187) |
| 17           | SpacTor-T5: Pre-training T5 Models with Span Corruption and Replaced Token Detection                                 | [Link](https://arxiv.org/abs/2401.13160) |
| 18           | A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity                               | [Link](https://arxiv.org/abs/2401.01967) |
| 19           | Mixtral of Experts                                                                                                  | [Link](https://arxiv.org/abs/2401.04088) |
| 20           | MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts                                           | [Link](https://arxiv.org/abs/2401.04081) |
| 21           | Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering                                       | [Link](https://arxiv.org/abs/2401.08500) |
| 22           | EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty                                                 | [Link](https://arxiv.org/abs/2401.15077) |
| 23           | KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization                                 | [Link](https://arxiv.org/abs/2401.18079) |
| 24           | A Closer Look at AUROC and AUPRC under Class Imbalance                                                              | [Link](https://arxiv.org/abs/2401.06091) |
| 25           | Transformers are Multi-State RNNs                                                                                   | [Link](https://arxiv.org/abs/2401.06104) |
| 26           | LLM Augmented LLMs: Expanding Capabilities through Composition                                                     | [Link](https://arxiv.org/abs/2401.02412) |
| 27           | Rethinking Patch Dependence for Masked Autoencoders                                                                 | [Link](https://arxiv.org/abs/2401.14391) |
| 28           | Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models                                         | [Link](https://arxiv.org/abs/2401.00788) |
| 29           | Pix2gestalt: Amodal Segmentation by Synthesizing Wholes                                                             | [Link](https://arxiv.org/abs/2401.14398) |
| 30           | RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture                                           | [Link](https://arxiv.org/abs/2401.08406) |
| 31           | An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models                | [Link](https://arxiv.org/abs/2401.06692) |
| 32           | Knowledge Fusion of Large Language Models                                                                          | [Link](https://arxiv.org/abs/2401.10491) |
| 33           | Scalable Pre-training of Large Autoregressive Image Models                                                          | [Link](https://arxiv.org/abs/2401.08541) |
| 34           | SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities                                     | [Link](https://arxiv.org/abs/2401.12168) |
| 35           | Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities                                 | [Link](https://arxiv.org/abs/2401.14405) |
| 36           | MambaByte: Token-free Selective State Space Model                                                                   | [Link](https://arxiv.org/abs/2401.13660) |
| 37           | ReFT: Reasoning with Reinforced Fine-Tuning                                                                        | [Link](https://arxiv.org/abs/2401.08967) |
| 38           | Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models                                       | [Link](https://arxiv.org/abs/2401.01335) |
| 39           | Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training                                        | [Link](https://arxiv.org/abs/2401.05566) |
| 40           | Denoising Vision Transformers                                                                                      | [Link](https://arxiv.org/abs/2401.02957) |
| 41           | Self-Rewarding Language Models                                                                                      | [Link](https://arxiv.org/abs/2401.10020) |
| 42           | LoRA+: Efficient Low Rank Adaptation of Large Models                                                                  | [Link](https://arxiv.org/abs/2402.12354) |
| 43           | MobileVLM V2: Faster and Stronger Baseline for Vision Language Model                                                  | [Link](https://arxiv.org/abs/2402.03766) |
| 44           | Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization? | [Link](https://arxiv.org/abs/2402.00841) |
| 45           | ODIN: Disentangled Reward Mitigates Hacking in RLHF                                                                   | [Link](https://arxiv.org/abs/2402.07319) |
| 46           | Genie: Generative Interactive Environments                                                                            | [Link](https://arxiv.org/abs/2402.15391) |
| 47           | A Phase Transition Between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention             | [Link](https://arxiv.org/abs/2402.03902) |
| 48           | Neural Network Diffusion                                                                                            | [Link](https://arxiv.org/abs/2402.13144) |
| 49           | More Agents Is All You Need                                                                                          | [Link](https://arxiv.org/abs/2402.05120) |
| 50           | Scaling Laws for Downstream Task Performance of Large Language Models                                                | [Link](https://arxiv.org/abs/2402.04177) |
|--------------|---------------------------------------------------------------------------------------------------------------------|------------------------------------|
| 51           | Repeat After Me: Transformers are Better than State Space Models at Copying                                         | [Link](https://arxiv.org/abs/2402.01032) |
| 52           | Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models                          | [Link](https://arxiv.org/abs/2402.19427) |
| 53           | AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling                                                      | [Link](https://arxiv.org/abs/2402.12226) |
| 54           | BASE TTS: Lessons From Building a Billion-Parameter Text-to-Speech Model on 100K Hours of Data                       | [Link](https://arxiv.org/abs/2402.08093) |
| 55           | LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration                                | [Link](https://arxiv.org/abs/2402.11550) |
| 56           | LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens                                                      | [Link](https://arxiv.org/abs/2402.13753) |
| 57           | Policy Improvement using Language Feedback Models                                                                   | [Link](https://arxiv.org/abs/2402.07876) |
| 58           | DoRA: Weight-Decomposed Low-Rank Adaptation                                                                          | [Link](https://arxiv.org/abs/2402.09353) |
| 59           | FindingEmo: An Image Dataset for Emotion Recognition in the Wild                                                    | [Link](https://arxiv.org/abs/2402.01355) |
| 60           | TinyLLaVA: A Framework of Small-scale Large Multimodal Models                                                       | [Link](https://arxiv.org/abs/2402.14289) |
| 61           | Vision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models                                   | [Link](https://arxiv.org/abs/2402.03749) |
| 62           | When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method                                   | [Link](https://arxiv.org/abs/2402.17193) |
| 63           | Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping                                               | [Link](https://arxiv.org/abs/2402.07610) |
| 64           | Suppressing Pink Elephants with Direct Principle Feedback                                                           | [Link](https://arxiv.org/abs/2402.07896) |
| 65           | The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits                                                   | [Link](https://arxiv.org/abs/2402.17764) |
| 66           | LiPO: Listwise Preference Optimization through Learning-to-Rank                                                    | [Link](https://arxiv.org/abs/2402.01878) |
| 67           | Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model                                         | [Link](https://arxiv.org/abs/2402.07610) |
| 68           | The Boundary of Neural Network Trainability is Fractal                                                             | [Link](https://arxiv.org/abs/2402.06184) |
| 69           | Recovering the Pre-Fine-Tuning Weights of Generative Models                                                        | [Link](https://arxiv.org/abs/2402.10208) |
| 70           | Scaling Laws for Fine-Grained Mixture of Experts                                                                   | [Link](https://arxiv.org/abs/2402.07871) |
| 71           | Direct Language Model Alignment from Online AI Feedback                                                            | [Link](https://arxiv.org/abs/2402.04792) |
| 72           | CARTE: Pretraining and Transfer for Tabular Learning                                                               | [Link](https://arxiv.org/abs/2402.16785) |
| 73           | Grandmaster-Level Chess Without Search                                                                             | [Link](https://arxiv.org/abs/2402.04494) |
| 74           | Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs                   | [Link](https://arxiv.org/abs/2402.14740) |
| 75           | Reformatted Alignment                                                                                              | [Link](https://arxiv.org/abs/2402.12219) |
| 76           | OLMo: Accelerating the Science of Language Models                                                                  | [Link](https://arxiv.org/abs/2402.00838) |
| 77           | FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models                                        | [Link](https://arxiv.org/abs/2402.10986) |
| 78           | Mixtures of Experts Unlock Parameter Scaling for Deep RL                                                          | [Link](https://arxiv.org/abs/2402.08609) |
| 79           | Generative Representational Instruction Tuning                                                                     | [Link](https://arxiv.org/abs/2402.09906) |
| 80           | World Model on Million-Length Video And Language With RingAttention                                               | [Link](https://arxiv.org/abs/2402.08268) |
| 81           | Efficient Exploration for LLMs                                                                                    | [Link](https://arxiv.org/abs/2402.00396) |
| 82           | YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information                                    | [Link](https://arxiv.org/abs/2402.13616) |
| 83           | Towards Cross-Tokenizer Distillation: The Universal Logit Distillation Loss for LLMs                                  | [Link](https://arxiv.org/abs/2402.12030) |
| 84           | Mixtures of Experts Unlock Parameter Scaling for Deep RL                                                             | [Link](https://arxiv.org/abs/2402.08609) |
| 85           | Sora Generates Videos with Stunning Geometrical Consistency                                                         | [Link](https://arxiv.org/abs/2402.17403) |
| 86           | BASE TTS: Lessons From Building a Billion-Parameter Text-to-Speech Model on 100K Hours of Data                       | [Link](https://arxiv.org/abs/2402.08093) |
| 87           | Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs                    | [Link](https://arxiv.org/abs/2402.14740) |
| 88           | Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models                         | [Link](https://arxiv.org/abs/2402.19427) |
| 89           | DoRA: Weight-Decomposed Low-Rank Adaptation                                                                          | [Link](https://arxiv.org/abs/2402.09353) |
| 90           | The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits                                                   | [Link](https://arxiv.org/abs/2402.17764) |
| 91           | Efficient Exploration for LLMs                                                                                      | [Link](https://arxiv.org/abs/2402.00396) |
| 92           | Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training                              | [Link](https://arxiv.org/abs/2405.15319) |
| 93           | You Only Cache Once: Decoder-Decoder Architectures for Language Models                                                | [Link](https://arxiv.org/abs/2405.05254) |
| 94           | gzip Predicts Data-dependent Scaling Laws                                                                             | [Link](https://arxiv.org/abs/2405.16684) |
| 95           | Self-Play Preference Optimization for Language Model Alignment                                                       | [Link](https://arxiv.org/abs/2405.00675) |
| 96           | PHUDGE: Phi-3 as Scalable Judge                                                                                      | [Link](https://arxiv.org/abs/2405.08029) |
| 97           | What Matters When Building Vision-Language Models?                                                                   | [Link](https://arxiv.org/abs/2405.02246) |
| 98           | Towards Modular LLMs by Building and Reusing a Library of LoRAs                                                      | [Link](https://arxiv.org/abs/2405.11157) |
| 99           | Contextual Position Encoding: Learning to Count What‚Äôs Important                                                     | [Link](https://arxiv.org/abs/2405.18719) |
| 100          | RLHF Workflow: From Reward Modeling to Online RLHF                                                                   | [Link](https://arxiv.org/abs/2405.07863) |
|--------------|-----------------------------------------------------------------------------------------------------------------------|-------------------------------------|
| 101          | Attention as an RNN                                                                                                  | [Link](https://arxiv.org/abs/2405.13956) |
| 102          | AlignGPT: Multi-modal Large Language Models with Adaptive Alignment Capability                                        | [Link](https://arxiv.org/abs/2405.14129) |
| 103          | Instruction Tuning With Loss Over Instructions                                                                       | [Link](https://arxiv.org/abs/2405.14394) |
| 104          | LoRA Learns Less and Forgets Less                                                                                    | [Link](https://arxiv.org/abs/2405.09673) |
| 105          | Trans-LoRA: Towards Data-free Transferable Parameter Efficient Finetuning                                            | [Link](https://arxiv.org/abs/2405.17258) |
| 106          | VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections                                                 | [Link](https://arxiv.org/abs/2405.17991) |
| 107          | MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning                                                         | [Link](https://arxiv.org/abs/2405.12130) |
| 108          | LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models                                            | [Link](https://arxiv.org/abs/2405.18377) |
| 109          | SimPO: Simple Preference Optimization with a Reference-Free Reward                                                   | [Link](https://arxiv.org/abs/2405.14734) |
| 110          | The Road Less Scheduled                                                                                             | [Link](https://arxiv.org/abs/2405.15682) |
| 111          | Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models                          | [Link](https://arxiv.org/abs/2405.05417) |
| 112          | Is Flash Attention Stable?                                                                                           | [Link](https://arxiv.org/abs/2405.02803) |
| 113          | Value Augmented Sampling for Language Model Alignment and Personalization                                            | [Link](https://arxiv.org/abs/2405.06639) |
| 114          | Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?                                                    | [Link](https://arxiv.org/abs/2405.05904) |
| 115          | vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention                                       | [Link](https://arxiv.org/abs/2405.04437) |
| 116          | A Careful Examination of Large Language Model Performance on Grade School Arithmetic                                 | [Link](https://arxiv.org/abs/2405.00332) |
| 117          | Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models                         | [Link](https://arxiv.org/abs/2405.01535) |
| 118          | DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model                                  | [Link](https://arxiv.org/abs/2405.04434) |
| 119          | Dense Connector for MLLMs                                                                                           | [Link](https://arxiv.org/abs/2405.13800) |
| 120          | xLSTM: Extended Long Short-Term Memory                                                                               | [Link](https://arxiv.org/abs/2405.04517) |
| 121          | SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization  | [Link](https://arxiv.org/abs/2405.11582) |
| 122          | Xmodel-VLM: A Simple Baseline for Multimodal Vision Language Model                                                  | [Link](https://arxiv.org/abs/2405.09215) |
| 123          | Chameleon: Mixed-Modal Early-Fusion Foundation Models                                                               | [Link](https://arxiv.org/abs/2405.09818) |
| 124          | Is Bigger Edit Batch Size Always Better? An Empirical Study on Model Editing with Llama-3                           | [Link](https://arxiv.org/abs/2405.00664) |
| 125          | AlignGPT: Multi-modal Large Language Models with Adaptive Alignment Capability                                       | [Link](https://arxiv.org/abs/2405.14129) |
| 126          | The Prompt Report: A Systematic Survey of Prompting Techniques                                                       | [Link](https://arxiv.org/abs/2406.06608) |
| 127          | Creativity Has Left the Chat: The Price of Debiasing Language Models                                                 | [Link](https://arxiv.org/abs/2406.05587) |
| 128          | Show, Don‚Äôt Tell: Aligning Language Models with Demonstrated Feedback                                                | [Link](https://arxiv.org/abs/2406.00888) |
| 129          | WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild                                      | [Link](https://arxiv.org/abs/2406.04770) |
| 130          | Scalable MatMul-free Language Modeling                                                                               | [Link](https://arxiv.org/abs/2406.02528) |
| 131          | Never Miss A Beat: An Efficient Recipe for Context Window Extension of Large Language Models                         | [Link](https://arxiv.org/abs/2406.07138) |
| 132          | Boosting Large-scale Parallel Training Efficiency with C4: A Communication-Driven Approach                           | [Link](https://arxiv.org/abs/2406.04594) |
| 133          | Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models                             | [Link](https://arxiv.org/abs/2406.06563) |
| 134          | MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding                                          | [Link](https://arxiv.org/abs/2406.09297) |
| 135          | Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling                            | [Link](https://arxiv.org/abs/2406.07522) |
| 136          | An Image is Worth 32 Tokens for Reconstruction and Generation                                                       | [Link](https://arxiv.org/abs/2406.07550) |
| 137          | Block Transformer: Global-to-Local Language Modeling for Fast Inference                                             | [Link](https://arxiv.org/abs/2406.02657) |
| 138          | 3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination                          | [Link](https://arxiv.org/abs/2406.05132) |
| 139          | Transformers Need Glasses! Information Over-Squashing in Language Tasks                                             | [Link](https://arxiv.org/abs/2406.04267) |
| 140          | The Geometry of Categorical and Hierarchical Concepts in Large Language Models                                      | [Link](https://arxiv.org/abs/2406.01506) |
| 141          | BERTs are Generative In-Context Learners                                                                            | [Link](https://arxiv.org/abs/2406.04823) |
| 142          | An Empirical Study of Mamba-based Language Models                                                                   | [Link](https://arxiv.org/abs/2406.07887) |
| 143          | Discovering Preference Optimization Algorithms with and for Large Language Models                                   | [Link](https://arxiv.org/abs/2406.08414) |
| 144          | Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing                               | [Link](https://arxiv.org/abs/2406.08464) |
| 145          | Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation                                           | [Link](https://arxiv.org/abs/2406.06525) |
| 146          | Are We Done with MMLU?                                                                                             | [Link](https://arxiv.org/abs/2406.04127) |
| 147          | OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models                                                    | [Link](https://arxiv.org/abs/2406.01775) |
| 148          | Step-aware Preference Optimization: Aligning Preference with Denoising Performance at Each Step                    | [Link](https://arxiv.org/abs/2406.04314) |
| 149          | Husky: A Unified, Open-Source Language Agent for Multi-Step Reasoning                                              | [Link](https://arxiv.org/abs/2406.06469) |
| 150          | Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters                                      | [Link](https://arxiv.org/abs/2406.05955) |
|--------------|-----------------------------------------------------------------------------------------------------------------------|-------------------------------------|
| 151          | Simple and Effective Masked Diffusion Language Models                                                              | [Link](https://arxiv.org/abs/2406.07524) |
| 152          | The Prompt Report: A Systematic Survey of Prompting Techniques                                                     | [Link](https://arxiv.org/abs/2406.06608) |
| 153          | Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs                                    | [Link](https://arxiv.org/abs/2406.10216) |
| 154          | Be like a Goldfish, Don‚Äôt Memorize! Mitigating Memorization in Generative LLMs                                      | [Link](https://arxiv.org/abs/2406.10209) |
| 155          | TextGrad: Automatic ‚ÄúDifferentiation‚Äù via Text                                                                     | [Link](https://arxiv.org/abs/2406.07496) |
| 156          | Large Language Models Must Be Taught to Know What They Don‚Äôt Know                                                  | [Link](https://arxiv.org/abs/2406.08391) |
| 157          | What If We Recaption Billions of Web Images with LLaMA-3?                                                         | [Link](https://arxiv.org/abs/2406.08478) |
| 158          | Discovering Preference Optimization Algorithms with and for Large Language Models                                  | [Link](https://arxiv.org/abs/2406.08414) |
| 159          | An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels                             | [Link](https://arxiv.org/abs/2406.09415) |
| 160          | Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models                                        | [Link](https://arxiv.org/abs/2406.04271) |
| 161          | CRAG ‚Äì Comprehensive RAG Benchmark                                                                                | [Link](https://arxiv.org/abs/2406.04744) |
| 162          | Margin-aware Preference Optimization for Aligning Diffusion Models Without Reference                               | [Link](https://arxiv.org/abs/2406.06424) |
| 163          | Mixture-of-Agents Enhances Large Language Model Capabilities                                                      | [Link](https://arxiv.org/abs/2406.04692) |
| 164          | Large Language Model Unlearning via Embedding-Corrupted Prompts                                                   | [Link](https://arxiv.org/abs/2406.07933) |
| 165          | Bootstrapping Language Models with DPO Implicit Rewards                                                           | [Link](https://arxiv.org/abs/2406.09760) |
| 166          | THEANINE: Revisiting Memory Management in Long-term Conversations with Timeline-augmented Response Generation         | [Link](https://arxiv.org/abs/2406.10996) |
| 167          | Task Me Anything                                                                                                     | [Link](https://arxiv.org/abs/2406.11775) |
| 168          | Nemotron-4 340B Technical Report                                                                                     | [Link](https://arxiv.org/abs/2406.11704) |
| 169          | Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges                                       | [Link](https://arxiv.org/abs/2406.12624) |
| 170          | How Do Large Language Models Acquire Factual Knowledge During Pretraining?                                           | [Link](https://arxiv.org/abs/2406.11813) |
| 171          | mDPO: Conditional Preference Optimization for Multimodal Large Language Models                                       | [Link](https://arxiv.org/abs/2406.11839) |
| 172          | Unveiling Encoder-Free Vision-Language Models                                                                        | [Link](https://arxiv.org/abs/2406.11832) |
| 173          | HARE: HumAn pRiors, a key to small language model Efficiency                                                         | [Link](https://arxiv.org/abs/2406.11410) |
| 174          | Measuring Memorization in RLHF for Code Completion                                                                   | [Link](https://arxiv.org/abs/2406.11715) |
| 175          | DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence                                 | [Link](https://arxiv.org/abs/2406.11931) |
| 176          | Iterative Length-Regularized Direct Preference Optimization: Improving 7B Language Models to GPT-4 Level             | [Link](https://arxiv.org/abs/2406.11817) |
| 177          | From RAGs to Rich Parameters: Probing How Language Models Utilize External Knowledge Over Parametric Information     | [Link](https://arxiv.org/abs/2406.12824) |
| 178          | DataComp-LM: In Search of the Next Generation of Training Sets for Language Models                                   | [Link](https://arxiv.org/abs/2406.11794) |
| 179          | Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?                                              | [Link](https://arxiv.org/abs/2406.13121) |
| 180          | Instruction Pre-Training: Language Models are Supervised Multitask Learners                                         | [Link](https://arxiv.org/abs/2406.14491) |
| 181          | Can LLMs Learn by Teaching? A Preliminary Study                                                                     | [Link](https://arxiv.org/abs/2406.14629) |
| 182          | A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems                                                 | [Link](https://arxiv.org/abs/2406.14972) |
| 183          | LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs                                            | [Link](https://arxiv.org/abs/2406.15319) |
| 184          | MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression                                      | [Link](https://arxiv.org/abs/2406.14909) |
| 185          | Efficient Continual Pre-training by Mitigating the Stability Gap                                                    | [Link](https://arxiv.org/abs/2406.14833) |
| 186          | Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers                          | [Link](https://arxiv.org/abs/2406.16747) |
| 187          | WARP: On the Benefits of Weight Averaged Rewarded Policies                                                          | [Link](https://arxiv.org/abs/2406.16768) |
| 188          | Adam-mini: Use Fewer Learning Rates To Gain More                                                                    | [Link](https://arxiv.org/abs/2406.16793) |
| 189          | The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale                                           | [Link](https://arxiv.org/abs/2406.17557) |
| 190          | LongIns: A Challenging Long-context Instruction-based Exam for LLMs                                                 | [Link](https://arxiv.org/abs/2406.17588) |
| 191          | Following Length Constraints in Instructions                                                                        | [Link](https://arxiv.org/abs/2406.17744) |
| 192          | A Closer Look into Mixture-of-Experts in Large Language Models                                                     | [Link](https://arxiv.org/abs/2406.18219) |
| 193          | RouteLLM: Learning to Route LLMs with Preference Data                                                               | [Link](https://arxiv.org/abs/2406.18665) |
| 194          | Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs                                        | [Link](https://arxiv.org/abs/2406.18629) |
| 195          | Dataset Size Recovery from LoRA Weights                                                                            | [Link](https://arxiv.org/abs/2406.19395) |
| 196          | From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data | [Link](https://arxiv.org/abs/2406.19292) |
| 197          | Changing Answer Order Can Decrease MMLU Accuracy                                                                   | [Link](https://arxiv.org/abs/2406.19470) |
| 198          | Direct Preference Knowledge Distillation for Large Language Models                                                 | [Link](https://arxiv.org/abs/2406.19774) |
| 199          | LLM Critics Help Catch LLM Bugs                                                                                    | [Link](https://arxiv.org/abs/2407.00215) |
| 200          | Scaling Synthetic Data Creation with 1,000,000,000 Personas                                                        | [Link](https://arxiv.org/abs/2406.20094) |
|--------------|-----------------------------------------------------------------------------------------------------------------------|-------------------------------------|
| 201          | Tokenization Falling Short: The Curse of Tokenization                                                              | [Link](https://arxiv.org/abs/2406.11687) |
| 202          | Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs                                    | [Link](https://arxiv.org/abs/2406.10216) |
| 203          | Bootstrapping Language Models with DPO Implicit Rewards                                                           | [Link](https://arxiv.org/abs/2406.09760) |
| 204          | Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges                                     | [Link](https://arxiv.org/abs/2406.12624) |
| 205          | Learning and Leveraging World Models in Visual Representation Learning                                                | [Link](https://arxiv.org/abs/2403.00504) |
| 206          | Improving LLM Code Generation with Grammar Augmentation                                                              | [Link](https://arxiv.org/abs/2403.01632) |
| 207          | The Hidden Attention of Mamba Models                                                                                 | [Link](https://arxiv.org/abs/2403.01590) |
| 208          | Training-Free Pretrained Model Merging                                                                               | [Link](https://arxiv.org/abs/2403.01753) |
| 209          | Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures                                   | [Link](https://arxiv.org/abs/2403.02308) |
| 210          | The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning                                             | [Link](https://arxiv.org/abs/2403.03218) |
| 211          | Evolution Transformer: In-Context Evolutionary Optimization                                                         | [Link](https://arxiv.org/abs/2403.02985) |
| 212          | Enhancing Vision-Language Pre-training with Rich Supervisions                                                       | [Link](https://arxiv.org/abs/2403.03346) |
| 213          | Scaling Rectified Flow Transformers for High-Resolution Image Synthesis                                             | [Link](https://arxiv.org/abs/2403.03206) |
| 214          | Design2Code: How Far Are We From Automating Front-End Engineering?                                                  | [Link](https://arxiv.org/abs/2403.03163) |
| 215          | ShortGPT: Layers in Large Language Models are More Redundant Than You Expect                                        | [Link](https://arxiv.org/abs/2403.03853) |
| 216          | Backtracing: Retrieving the Cause of the Query                                                                      | [Link](https://arxiv.org/abs/2403.03956) |
| 217          | Learning to Decode Collaboratively with Multiple Language Models                                                   | [Link](https://arxiv.org/abs/2403.03870) |
| 218          | SaulLM-7B: A pioneering Large Language Model for Law                                                               | [Link](https://arxiv.org/abs/2403.03883) |
| 219          | Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning        | [Link](https://arxiv.org/abs/2403.03864) |
| 220          | 3D Diffusion Policy                                                                                                 | [Link](https://arxiv.org/abs/2403.03954) |
| 221          | MedMamba: Vision Mamba for Medical Image Classification                                                            | [Link](https://arxiv.org/abs/2403.03849) |
| 222          | GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection                                              | [Link](https://arxiv.org/abs/2403.03507) |
| 223          | Stop Regressing: Training Value Functions via Classification for Scalable Deep RL                                  | [Link](https://arxiv.org/abs/2403.03950) |
| 224          | How Far Are We from Intelligent Visual Deductive Reasoning?                                                        | [Link](https://arxiv.org/abs/2403.04732) |
| 225          | Common 7B Language Models Already Possess Strong Math Capabilities                                                | [Link](https://arxiv.org/abs/2403.04706) |
| 226          | Gemini 1.5: Unlocking Multimodal Understanding Across Millions of Tokens of Context                                | [Link](https://arxiv.org/abs/2403.05530) |
| 227          | Is Cosine-Similarity of Embeddings Really About Similarity?                                                       | [Link](https://arxiv.org/abs/2403.05440) |
| 228          | LLM4Decompile: Decompiling Binary Code with Large Language Models                                                 | [Link](https://arxiv.org/abs/2403.05286) |
| 229          | Algorithmic Progress in Language Models                                                                           | [Link](https://arxiv.org/abs/2403.05812) |
| 230          | Stealing Part of a Production Language Model                                                                      | [Link](https://arxiv.org/abs/2403.06634) |
| 231          | Chronos: Learning the Language of Time Series                                                                     | [Link](https://arxiv.org/abs/2403.07815) |
| 232          | Simple and Scalable Strategies to Continually Pre-train Large Language Models                                     | [Link](https://arxiv.org/abs/2403.08763) |
| 233          | Language Models Scale Reliably With Over-Training and on Downstream Tasks                                         | [Link](https://arxiv.org/abs/2403.08540) |
| 234          | BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences                         | [Link](https://arxiv.org/abs/2403.09347) |
| 235          | LocalMamba: Visual State Space Model with Windowed Selective Scan                                                | [Link](https://arxiv.org/abs/2403.09338) |
| 236          | GiT: Towards Generalist Vision Transformer through Universal Language Interface                                   | [Link](https://arxiv.org/abs/2403.09394) |
| 237          | MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training                                               | [Link](https://arxiv.org/abs/2403.09611) |
| 238          | RAFT: Adapting Language Model to Domain Specific RAG                                                             | [Link](https://arxiv.org/abs/2403.10131) |
| 239          | TnT-LLM: Text Mining at Scale with Large Language Models                                                         | [Link](https://arxiv.org/abs/2403.12173) |
| 240          | Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression                   | [Link](https://arxiv.org/abs/2403.15447) |
| 241          | PERL: Parameter Efficient Reinforcement Learning from Human Feedback                                             | [Link](https://arxiv.org/abs/2403.10704) |
| 242          | RewardBench: Evaluating Reward Models for Language Modeling                                                      | [Link](https://arxiv.org/abs/2403.13787) |
| 243          | LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models                                              | [Link](https://arxiv.org/abs/2403.13372) |
| 244          | RakutenAI-7B: Extending Large Language Models for Japanese                                                       | [Link](https://arxiv.org/abs/2403.15484) |
| 245          | SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time Series                               | [Link](https://arxiv.org/abs/2403.15360) |
| 246          | Can Large Language Models Explore In-Context?                                                                   | [Link](https://arxiv.org/abs/2403.15371) |
| 247          | LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement                                                    | [Link](https://arxiv.org/abs/2403.15042) |
| 248          | LLM Agent Operating System                                                                                      | [Link](https://arxiv.org/abs/2403.16971) |
| 249          | The Unreasonable Ineffectiveness of the Deeper Layers                                                           | [Link](https://arxiv.org/abs/2403.17887) |
| 250          | BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text                                            | [Link](https://arxiv.org/abs/2403.18421) |
|--------------|-----------------------------------------------------------------------------------------------------------------------|-------------------------------------|
| 251          | ViTAR: Vision Transformer with Any Resolution                                                                   | [Link](https://arxiv.org/abs/2403.18361) |
| 252          | Long-form Factuality in Large Language Models                                                                   | [Link](https://arxiv.org/abs/2403.18802) |
| 253          | Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models                                      | [Link](https://arxiv.org/abs/2403.18814) |
| 254          | LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning                       | [Link](https://arxiv.org/abs/2403.17919) |
| 255          | Mechanistic Design and Scaling of Hybrid Architectures                                                         | [Link](https://arxiv.org/abs/2403.17844) |
| 256          | MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions                                         | [Link](https://arxiv.org/abs/2403.19651) |
| 257          | Model Stock: All We Need Is Just a Few Fine-Tuned Models                                                        | [Link](https://arxiv.org/abs/2403.19522) |
| 258          | Do Language Models Plan Ahead for Future Tokens?                                                                      | [Link](https://arxiv.org/abs/2404.00859) |
| 259          | Bigger is not Always Better: Scaling Properties of Latent Diffusion Models                                           | [Link](https://arxiv.org/abs/2404.01367) |
| 260          | The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis                   | [Link](https://arxiv.org/abs/2404.01204) |
| 261          | Diffusion-RWKV: Scaling RWKV-Like Architectures for Diffusion Models                                                 | [Link](https://arxiv.org/abs/2404.04478) |
| 262          | Mixture-of-Depths: Dynamically Allocating Compute in Transformer-Based Language Models                               | [Link](https://arxiv.org/abs/2404.02258) |
| 263          | Long-context LLMs Struggle with Long In-context Learning                                                             | [Link](https://arxiv.org/abs/2404.02060) |
| 264          | Emergent Abilities in Reduced-Scale Generative Language Models                                                      | [Link](https://arxiv.org/abs/2404.02204) |
| 265          | Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks                                              | [Link](https://arxiv.org/abs/2404.02151) |
| 266          | On the Scalability of Diffusion-based Text-to-Image Generation                                                      | [Link](https://arxiv.org/abs/2404.02883) |
| 267          | BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models                                  | [Link](https://arxiv.org/abs/2404.02827) |
| 268          | Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion Models                                        | [Link](https://arxiv.org/abs/2404.02747) |
| 269          | Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences                         | [Link](https://arxiv.org/abs/2404.02151) |
| 270          | Training LLMs over Neurally Compressed Text                                                                        | [Link](https://arxiv.org/abs/2404.03626) |
| 271          | CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues                                           | [Link](https://arxiv.org/abs/2404.03820) |
| 272          | ReFT: Representation Finetuning for Language Models                                                                | [Link](https://arxiv.org/abs/2404.03592) |
| 273          | Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data                                      | [Link](https://arxiv.org/abs/2404.03862) |
| 274          | Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation                                                 | [Link](https://arxiv.org/abs/2404.04256) |
| 275          | AutoCodeRover: Autonomous Program Improvement                                                                      | [Link](https://arxiv.org/abs/2404.05427) |
| 276          | Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence                                             | [Link](https://arxiv.org/abs/2404.05892) |
| 277          | CodecLM: Aligning Language Models with Tailored Synthetic Data                                                     | [Link](https://arxiv.org/abs/2404.05875) |
| 278          | MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies                        | [Link](https://arxiv.org/abs/2404.06395) |
| 279          | Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models                         | [Link](https://arxiv.org/abs/2404.06209) |
| 280          | LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders                                                | [Link](https://arxiv.org/abs/2404.05961) |
| 281          | Adapting LLaMA Decoder to Vision Transformer                                                                       | [Link](https://arxiv.org/abs/2404.06773) |
| 282          | Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention                             | [Link](https://arxiv.org/abs/2404.07143) |
| 283          | LLoCO: Learning Long Contexts Offline                                                                             | [Link](https://arxiv.org/abs/2404.07979) |
| 284          | JetMoE: Reaching Llama2 Performance with 0.1M Dollars                                                              | [Link](https://arxiv.org/abs/2404.07413) |
| 285          | Best Practices and Lessons Learned on Synthetic Data for Language Models                                          | [Link](https://arxiv.org/abs/2404.07503) |
| 286          | Rho-1: Not All Tokens Are What You Need                                                                           | [Link](https://arxiv.org/abs/2404.07965) |
| 287          | Pre-training Small Base LMs with Fewer Tokens                                                                     | [Link](https://arxiv.org/abs/2404.08634) |
| 288          | Dataset Reset Policy Optimization for RLHF                                                                           | [Link](https://arxiv.org/abs/2404.08495) |
| 289          | LLM In-Context Recall is Prompt Dependent                                                                            | [Link](https://arxiv.org/abs/2404.08865) |
| 290          | State Space Model for New-Generation Network Alternative to Transformers: A Survey                                   | [Link](https://arxiv.org/abs/2404.09516) |
| 291          | Chinchilla Scaling: A Replication Attempt                                                                            | [Link](https://arxiv.org/abs/2404.10102) |
| 292          | Learn Your Reference Model for Real Good Alignment                                                                  | [Link](https://arxiv.org/abs/2404.09656) |
| 293          | Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study                                                     | [Link](https://arxiv.org/abs/2404.10719) |
| 294          | Scaling (Down) CLIP: A Comprehensive Analysis of Data, Architecture, and Training Strategies                        | [Link](https://arxiv.org/abs/2404.08197) |
| 295          | How Faithful Are RAG Models? Quantifying the Tug-of-War Between RAG and LLMs‚Äô Internal Prior                        | [Link](https://arxiv.org/abs/2404.10198) |
| 296          | A Survey on Retrieval-Augmented Text Generation for Large Language Models                                           | [Link](https://arxiv.org/abs/2404.10981) |
| 297          | When LLMs are Unfit Use FastFit: Fast and Effective Text Classification with Many Classes                            | [Link](https://arxiv.org/abs/2404.12365) |
| 298          | Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing                                         | [Link](https://arxiv.org/abs/2404.12253) |
| 299          | OpenBezoar: Small, Cost-Effective and Open Models Trained on Mixes of Instruction Data                               | [Link](https://arxiv.org/abs/2404.12195) |
| 300          | The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions                                      | [Link](https://arxiv.org/abs/2404.13208) |
|--------------|-----------------------------------------------------------------------------------------------------------------------|-------------------------------------|
| 301          | How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study                                                    | [Link](https://arxiv.org/abs/2404.14047) |
| 302          | Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone                                       | [Link](https://arxiv.org/abs/2404.14219) |
| 303          | OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework                       | [Link](https://arxiv.org/abs/2404.14619) |
| 304          | A Survey on Self-Evolution of Large Language Models                                                                 | [Link](https://arxiv.org/abs/2404.14662) |
| 305          | Multi-Head Mixture-of-Experts                                                                                       | [Link](https://arxiv.org/abs/2404.15045) |
| 306          | NExT: Teaching Large Language Models to Reason about Code Execution                                                | [Link](https://arxiv.org/abs/2404.14662) |
| 307          | Graph Machine Learning in the Era of Large Language Models (LLMs)                                                  | [Link](https://arxiv.org/abs/2404.14928) |
| 308          | Retrieval Head Mechanistically Explains Long-Context Factuality                                                    | [Link](https://arxiv.org/abs/2404.15574) |
| 309          | Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding                                            | [Link](https://arxiv.org/abs/2404.16710) |
| 310          | Make Your LLM Fully Utilize the Context                                                                            | [Link](https://arxiv.org/abs/2404.16811) |
| 311          | LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report                                                | [Link](https://arxiv.org/abs/2405.00732) |
| 312          | Better & Faster Large Language Models via Multi-token Prediction                                                  | [Link](https://arxiv.org/abs/2404.19737) |
| 313          | RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing                         | [Link](https://arxiv.org/abs/2404.19543) |
| 314          | A Primer on the Inner Workings of Transformer-based Language Models                                                | [Link](https://arxiv.org/abs/2405.00208) |
| 315          | When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively                                       | [Link](https://arxiv.org/abs/2404.19705) |
| 316          | KAN: Kolmogorov‚ÄìArnold Networks                                                                                   | [Link](https://arxiv.org/abs/2404.19756) |
| 317          | LLM See, LLM Do: Guiding Data Generation to Target Non-Differentiable Objectives                                      | [Link](https://arxiv.org/abs/2407.01490) |
| 318          | Searching for Best Practices in Retrieval-Augmented Generation                                                      | [Link](https://arxiv.org/abs/2407.01219) |
| 319          | Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models       | [Link](https://arxiv.org/abs/2407.01906) |
| 320          | Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion                                               | [Link](https://arxiv.org/abs/2407.01392) |
| 321          | Eliminating Position Bias of Language Models: A Mechanistic Approach                                                | [Link](https://arxiv.org/abs/2407.01100) |
| 322          | JMInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention                         | [Link](https://arxiv.org/abs/2407.02490) |
| 323          | TokenPacker: Efficient Visual Projector for Multimodal LLM                                                          | [Link](https://arxiv.org/abs/2407.02392) |
| 324          | Reasoning in Large Language Models: A Geometric Perspective                                                         | [Link](https://arxiv.org/abs/2407.02678) |
| 325          | RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs                                       | [Link](https://arxiv.org/abs/2407.02485) |
| 326          | AgentInstruct: Toward Generative Teaching with Agentic Flows                                                        | [Link](https://arxiv.org/abs/2407.03502) |
| 327          | HEMM: Holistic Evaluation of Multimodal Foundation Models                                                           | [Link](https://arxiv.org/abs/2407.03418) |
| 328          | Mixture of A Million Experts                                                                                        | [Link](https://arxiv.org/abs/2407.04153) |
| 329          | Learning to (Learn at Test Time): RNNs with Expressive Hidden States                                                | [Link](https://arxiv.org/abs/2407.04620) |
| 330          | Vision Language Models Are Blind                                                                                    | [Link](https://arxiv.org/abs/2407.06581) |
| 331          | Self-Recognition in Language Models                                                                                 | [Link](https://arxiv.org/abs/2407.06946) |
| 332          | Inference Performance Optimization for Large Language Models on CPUs                                                | [Link](https://arxiv.org/abs/2407.07304) |
| 333          | Gradient Boosting Reinforcement Learning                                                                            | [Link](https://arxiv.org/abs/2407.08250) |
| 334          | FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision                                     | [Link](https://arxiv.org/abs/2407.08608) |
| 335          | SpreadsheetLLM: Encoding Spreadsheets for Large Language Models                                                     | [Link](https://arxiv.org/abs/2407.09025) |
| 336          | New Desiderata for Direct Preference Optimization                                                                   | [Link](https://arxiv.org/abs/2407.09072) |
| 337          | Context Embeddings for Efficient Answer Generation in RAG                                                          | [Link](https://arxiv.org/abs/2407.09252) |
| 338          | Qwen2 Technical Report                                                                                              | [Link](https://arxiv.org/abs/2407.10671) |
| 339          | The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism                             | [Link](https://arxiv.org/abs/2407.10457) |
| 340          | From GaLore to WeLore: How Low-Rank Weights Non-uniformly Emerge from Low-Rank Gradients                            | [Link](https://arxiv.org/abs/2407.11239) |
| 341          | GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill and Extreme KV-Cache Compression           | [Link](https://arxiv.org/abs/2407.12077) |
| 342          | Scaling Diffusion Transformers to 16 Billion Parameters                                                            | [Link](https://arxiv.org/abs/2407.11633) |
| 343          | NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?                                       | [Link](https://arxiv.org/abs/2407.11963) |
| 344          | Patch-Level Training for Large Language Models                                                                     | [Link](https://arxiv.org/abs/2407.12665) |
| 345          | LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models                                              | [Link](https://arxiv.org/abs/2407.12772) |
| 346          | A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks                            | [Link](https://arxiv.org/abs/2407.12994) |
| 347          | Spectra: A Comprehensive Study of Ternary, Quantized, and FP16 Language Models                                      | [Link](https://arxiv.org/abs/2407.12327) |
| 348          | Attention Overflow: Language Model Input Blur during Long-Context Missing Items Recommendation                     | [Link](https://arxiv.org/abs/2407.13481) |
| 349          | Weak-to-Strong Reasoning                                                                                           | [Link](https://arxiv.org/abs/2407.13647) |
| 350          | Understanding Reference Policies in Direct Preference Optimization                                                 | [Link](https://arxiv.org/abs/2407.13709) |
|--------------|-----------------------------------------------------------------------------------------------------------------------|-------------------------------------|
| 351          | Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies                                            | [Link](https://arxiv.org/abs/2407.13623) |
| 352          | BOND: Aligning LLMs with Best-of-N Distillation                                                                    | [Link](https://arxiv.org/abs/2407.14622) |
| 353          | Compact Language Models via Pruning and Knowledge Distillation                                                    | [Link](https://arxiv.org/abs/2407.14679) |
| 354          | LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference                                           | [Link](https://arxiv.org/abs/2407.14057) |
| 355          | Mini-Sequence Transformer: Optimizing Intermediate Memory for Long Sequences Training                              | [Link](https://arxiv.org/abs/2407.15892) |
| 356          | DDK: Distilling Domain Knowledge for Efficient Large Language Models                                              | [Link](https://arxiv.org/abs/2407.16154) |
| 357          | Generation Constraint Scaling Can Mitigate Hallucination                                                          | [Link](https://arxiv.org/abs/2407.16908) |
| 358          | Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach                    | [Link](https://arxiv.org/abs/2407.16833) |
| 359          | Course-Correction: Safety Alignment Using Synthetic Preferences                                                   | [Link](https://arxiv.org/abs/2407.16637) |
| 360          | Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?                                  | [Link](https://arxiv.org/abs/2407.16607) |
| 361          | Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge                                 | [Link](https://arxiv.org/abs/2407.19594) |
| 362          | Improving Retrieval Augmented Language Model with Self-Reasoning                                                  | [Link](https://arxiv.org/abs/2407.19813) |
| 363          | Apple Intelligence Foundation Language Models                                                                     | [Link](https://arxiv.org/abs/2407.21075) |
| 364          | ThinK: Thinner Key Cache by Query-Driven Pruning                                                                  | [Link](https://arxiv.org/abs/2407.21018) |
| 365          | The Llama 3 Herd of Models                                                                                       | [Link](https://arxiv.org/abs/2407.21783) |
| 366          | Gemma 2: Improving Open Language Models at a Practical Size                                                      | [Link](https://arxiv.org/abs/2408.00118) |
| 367          | SAM 2: Segment Anything in Images and Videos                                                                         | [Link](https://arxiv.org/abs/2408.00714) |
| 368          | POA: Pre-training Once for Models of All Sizes                                                                      | [Link](https://arxiv.org/abs/2408.01031) |
| 369          | RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework                                              | [Link](https://arxiv.org/abs/2408.01262) |
| 370          | A Survey of Mamba                                                                                                   | [Link](https://arxiv.org/abs/2408.01129) |
| 371          | MiniCPM-V: A GPT-4V Level MLLM on Your Phone                                                                       | [Link](https://arxiv.org/abs/2408.01800) |
| 372          | RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation                                      | [Link](https://arxiv.org/abs/2408.02545) |
| 373          | Self-Taught Evaluators                                                                                             | [Link](https://arxiv.org/abs/2408.02666) |
| 374          | BioMamba: A Pre-trained Biomedical Language Representation Model Leveraging Mamba                                  | [Link](https://arxiv.org/abs/2408.02600) |
| 375          | EXAONE 3.0 7.8B Instruction Tuned Language Model                                                                   | [Link](https://arxiv.org/abs/2408.03541) |
| 376          | 1.5-Pints Technical Report: Pretraining in Days, Not Months ‚Äì Your Language Model Thrives on Quality Data           | [Link](https://arxiv.org/abs/2408.03506) |
| 377          | Conversational Prompt Engineering                                                                                  | [Link](https://arxiv.org/abs/2408.04560) |
| 378          | Trans-Tokenization and Cross-lingual Vocabulary Transfers: Language Adaptation of LLMs for Low-Resource NLP         | [Link](https://arxiv.org/abs/2408.04303) |
| 379          | The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery                                          | [Link](https://arxiv.org/abs/2408.06292) |
| 380          | Hermes 3 Technical Report                                                                                         | [Link](https://arxiv.org/abs/2408.12570) |
| 381          | Customizing Language Models with Instance-wise LoRA for Sequential Recommendation                                  | [Link](https://arxiv.org/abs/2408.10159) |
| 382          | Enhancing Robustness in Large Language Models: Prompting for Mitigating the Impact of Irrelevant Information        | [Link](https://arxiv.org/abs/2408.10615) |
| 383          | To Code, or Not To Code? Exploring Impact of Code in Pre-training                                                 | [Link](https://arxiv.org/abs/2408.10914) |
| 384          | LLM Pruning and Distillation in Practice: The Minitron Approach                                                    | [Link](https://arxiv.org/abs/2408.11796) |
| 385          | Jamba-1.5: Hybrid Transformer-Mamba Models at Scale                                                               | [Link](https://arxiv.org/abs/2408.12570) |
| 386          | Controllable Text Generation for Large Language Models: A Survey                                                  | [Link](https://arxiv.org/abs/2408.12599) |
| 387          | Multi-Layer Transformers Gradient Can be Approximated in Almost Linear Time                                       | [Link](https://arxiv.org/abs/2408.13233) |
| 388          | A Practitioner‚Äôs Guide to Continual Multimodal Pretraining                                                        | [Link](https://arxiv.org/abs/2408.14471) |
| 389          | Building and Better Understanding Vision-Language Models: Insights and Future Directions                          | [Link](https://arxiv.org/abs/2408.12637) |
| 390          | CURLoRA: Stable LLM Continual Fine-Tuning and Catastrophic Forgetting Mitigation                                  | [Link](https://arxiv.org/abs/2408.14572) |
| 391          | The Mamba in the Llama: Distilling and Accelerating Hybrid Models                                                 | [Link](https://arxiv.org/abs/2408.15237) |
| 392          | ReMamba: Equip Mamba with Effective Long-Sequence Modeling                                                        | [Link](https://arxiv.org/abs/2408.15496) |
| 393          | Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling                                  | [Link](https://arxiv.org/abs/2408.16737) |
| 394          | LongRecipe: Recipe for Efficient Long Context Generalization in Large Language Models                             | [Link](https://arxiv.org/abs/2409.00509) |
| 395          | Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis                                                | [Link](https://arxiv.org/abs/2412.01819) |
| 396          | X-Prompt: Towards Universal In-Context Image Generation in Auto-Regressive Vision Language Foundation Models          | [Link](https://arxiv.org/abs/2412.01824) |
| 397          | Free Process Rewards without Process Labels                                                                          | [Link](https://arxiv.org/abs/2412.01981) |
| 398          | Scaling Image Tokenizers with Grouped Spherical Quantization                                                        | [Link](https://arxiv.org/abs/2412.02632) |
| 399          | RARE: Retrieval-Augmented Reasoning Enhancement for Large Language Models                                           | [Link](https://arxiv.org/abs/2412.02830) |
| 400          | Perception Tokens Enhance Visual Reasoning in Multimodal Language Models                                            | [Link](https://arxiv.org/abs/2412.03548) |
|--------------|-----------------------------------------------------------------------------------------------------------------------|-------------------------------------|
| 401          | Evaluating Language Models as Synthetic Data Generators                                                             | [Link](https://arxiv.org/abs/2412.03679) |
| 402          | Best-of-N Jailbreaking                                                                                              | [Link](https://arxiv.org/abs/2412.03556) |
| 403          | PaliGemma 2: A Family of Versatile VLMs for Transfer                                                                | [Link](https://arxiv.org/abs/2412.03555) |
| 404          | VisionZip: Longer is Better but Not Necessary in Vision Language Models                                             | [Link](https://arxiv.org/abs/2412.04467) |
| 405          | Evaluating and Aligning CodeLLMs on Human Preference                                                                | [Link](https://arxiv.org/abs/2412.05210) |
| 406          | MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale                                         | [Link](https://arxiv.org/abs/2412.05237) |
| 407          | Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling           | [Link](https://arxiv.org/abs/2412.05271) |
| 408          | LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods                                              | [Link](https://arxiv.org/abs/2412.05579) |
| 409          | Does RLHF Scale? Exploring the Impacts From Data, Model, and Method                                                | [Link](https://arxiv.org/abs/2412.06000) |
| 410          | Unraveling the Complexity of Memory in RL Agents: An Approach for Classification and Evaluation                     | [Link](https://arxiv.org/abs/2412.06531) |
| 411          | Training Large Language Models to Reason in a Continuous Latent Space                                              | [Link](https://arxiv.org/abs/2412.06769) |
| 412          | AutoReason: Automatic Few-Shot Reasoning Decomposition                                                             | [Link](https://arxiv.org/abs/2412.06975) |
| 413          | Large Concept Models: Language Modeling in a Sentence Representation Space                                         | [Link](https://arxiv.org/abs/2412.08821) |
| 414          | Phi-4 Technical Report                                                                                            | [Link](https://arxiv.org/abs/2412.08905) |
| 415          | Byte Latent Transformer: Patches Scale Better Than Tokens                                                          | [Link](https://arxiv.org/abs/2412.09871) |
| 416          | SCBench: A KV Cache-Centric Analysis of Long-Context Methods                                                      | [Link](https://arxiv.org/abs/2412.10319) |
| 417          | Cultural Evolution of Cooperation among LLM Agents                                                                | [Link](https://arxiv.org/abs/2412.10270) |
| 418          | DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding                      | [Link](https://arxiv.org/abs/2412.10302) |
| 419          | No More Adam: Learning Rate Scaling at Initialization is All You Need                                              | [Link](https://arxiv.org/abs/2412.11768) |
| 420          | Precise Length Control in Large Language Models                                                                   | [Link](https://arxiv.org/abs/2412.11937) |
| 421          | The Open Source Advantage in Large Language Models (LLMs)                                                         | [Link](https://arxiv.org/abs/2412.12004) |
| 422          | A Survey of Mathematical Reasoning in the Era of Multimodal Large Language Model: Benchmark, Method & Challenges   | [Link](https://arxiv.org/abs/2412.11936) |
| 423          | Are Your LLMs Capable of Stable Reasoning?                                                                        | [Link](https://arxiv.org/abs/2412.13147) |
| 424          | LLM Post-Training Recipes: Improving Reasoning in LLMs                                                            | [Link](https://arxiv.org/abs/2412.14135) |
| 425          | Hansel: Output Length Controlling Framework for Large Language Models                                             | [Link](https://arxiv.org/abs/2412.14033) |
| 426          | Mind Your Theory: Theory of Mind Goes Deeper Than Reasoning                                                       | [Link](https://arxiv.org/abs/2412.1363) |
| 427          | Alignment Faking in Large Language Models                                                                         | [Link](https://arxiv.org/abs/2412.14093) |
| 428          | SCOPE: Optimizing Key-Value Cache Compression in Long-Context Generation                                          | [Link](https://arxiv.org/abs/2412.13649) |
| 429          | LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-Context Multitasks                     | [Link](https://arxiv.org/abs/2412.15204) |
| 430          | Offline Reinforcement Learning for LLM Multi-Step Reasoning                                                      | [Link](https://arxiv.org/abs/2412.16145) |
| 431          | Mulberry: Empowering MLLM with O1-like Reasoning and Reflection via Collective Monte Carlo Tree Search            | [Link](https://arxiv.org/abs/2412.18319) |
| 432          | Titans: Learning to Memorize at Test Time                                                                         | [Link](https://arxiv.org/abs/2501.00663) |
| 433          | Addition is All You Need for Energy-efficient Language Models                                                        | [Link](https://arxiv.org/abs/2410.00907) |
| 434          | Quantifying Generalization Complexity for Large Language Models                                                     | [Link](https://arxiv.org/abs/2410.01769) |
| 435          | When a Language Model is Optimized for Reasoning, Does It Still Show Embers of Autoregression?                       | [Link](https://arxiv.org/abs/2410.01792) |
| 436          | Were RNNs All We Needed?                                                                                            | [Link](https://arxiv.org/abs/2410.01201) |
| 437          | Selective Attention Improves Transformer                                                                            | [Link](https://arxiv.org/abs/2410.02703) |
| 438          | LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations                                | [Link](https://arxiv.org/abs/2410.02707) |
| 439          | LLaVA-Critic: Learning to Evaluate Multimodal Models                                                                | [Link](https://arxiv.org/abs/2410.02712) |
| 440          | Differential Transformer                                                                                           | [Link](https://arxiv.org/abs/2410.05258) |
| 441          | GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models                     | [Link](https://arxiv.org/abs/2410.05229) |
| 442          | ARIA: An Open Multimodal Native Mixture-of-Experts Model                                                           | [Link](https://arxiv.org/abs/2410.05993) |
| 443          | O1 Replication Journey: A Strategic Progress Report ‚Äì Part 1                                                      | [Link](https://arxiv.org/abs/2410.18982) |
| 444          | Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG                                           | [Link](https://arxiv.org/abs/2410.05983) |
| 445          | From Generalist to Specialist: Adapting Vision Language Models via Task-Specific Visual Instruction Tuning          | [Link](https://arxiv.org/abs/2410.06456) |
| 446          | KV Prediction for Improved Time to First Token                                                                     | [Link](https://arxiv.org/abs/2410.08391) |
| 447          | Baichuan-Omni Technical Report                                                                                    | [Link](https://arxiv.org/abs/2410.08565) |
| 448          | MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models                      | [Link](https://arxiv.org/abs/2410.10139) |
| 449          | LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models                             | [Link](https://arxiv.org/abs/2410.09732) |
| 450          | AFlow: Automating Agentic Workflow Generation                                                                      | [Link](https://arxiv.org/abs/2410.10762) |
|--------------|--------------------------------------------------------------------------------------------------------------------|-------------------------------------|
| 451          | Toward General Instruction-Following Alignment for Retrieval-Augmented Generation                                  | [Link](https://arxiv.org/abs/2410.09584) |
| 452          | Pre-training Distillation for Large Language Models: A Design Space Exploration                                    | [Link](https://arxiv.org/abs/2410.16215) |
| 453          | MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models                     | [Link](https://arxiv.org/abs/2410.17637) |
| 454          | Scalable Ranked Preference Optimization for Text-to-Image Generation                                               | [Link](https://arxiv.org/abs/2410.18013) |
| 455          | Scaling Diffusion Language Models via Adaptation from Autoregressive Models                                        | [Link](https://arxiv.org/abs/2410.17891) |
| 456          | Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback                                          | [Link](https://arxiv.org/abs/2410.19133) |
| 457          | Counting Ability of Large Language Models and Impact of Tokenization                                               | [Link](https://arxiv.org/abs/2410.19730) |
| 458          | A Survey of Small Language Models                                                                                 | [Link](https://arxiv.org/abs/2410.20011) |
| 459          | Accelerating Direct Preference Optimization with Prefix Sharing                                                   | [Link](https://arxiv.org/abs/2410.20305) |
| 460          | Mind Your Step (by Step): Chain-of-Thought Can Reduce Performance on Tasks Where Thinking Makes Humans Worse       | [Link](https://arxiv.org/abs/2410.21333) |
| 461          | LongReward: Improving Long-context Large Language Models with AI Feedback                                         | [Link](https://arxiv.org/abs/2410.21252) |
| 462          | ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference                                      | [Link](https://arxiv.org/abs/2410.21465) |
| 463          | Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial Applications                                     | [Link](https://arxiv.org/abs/2410.21943) |
| 464          | CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation                                   | [Link](https://arxiv.org/abs/2410.23090) |
| 465          | What Happened in LLMs Layers When Trained for Fast vs. Slow Thinking: A Gradient Perspective                      | [Link](https://arxiv.org/abs/2410.23743) |
| 466          | GPT or BERT: Why Not Both?                                                                                        | [Link](https://arxiv.org/abs/2410.24159) |
| 467          | Language Models Can Self-Lengthen to Generate Long Texts                                                         | [Link](https://arxiv.org/abs/2410.23933) |
| 468          | OLMoE: Open Mixture-of-Experts Language Models                                                                       | [Link](https://arxiv.org/abs/2409.02060) |
| 469          | In Defense of RAG in the Era of Long-Context Language Models                                                        | [Link](https://arxiv.org/abs/2409.01666) |
| 470          | Attention Heads of Large Language Models: A Survey                                                                  | [Link](https://arxiv.org/abs/2409.03752) |
| 471          | LongCite: Enabling LLMs to Generate Fine-grained Citations in Long-context QA                                       | [Link](https://arxiv.org/abs/2409.02897) |
| 472          | How Do Your Code LLMs Perform? Empowering Code Instruction Tuning with High-Quality Data                            | [Link](https://arxiv.org/abs/2409.03810) |
| 473          | Theory, Analysis, and Best Practices for Sigmoid Self-Attention                                                    | [Link](https://arxiv.org/abs/2409.04431) |
| 474          | LLaMA-Omni: Seamless Speech Interaction with Large Language Models                                                 | [Link](https://arxiv.org/abs/2409.06666) |
| 475          | What is the Role of Small Models in the LLM Era: A Survey                                                          | [Link](https://arxiv.org/abs/2409.06857) |
| 476          | Policy Filtration in RLHF to Fine-Tune LLM for Code Generation                                                     | [Link](https://arxiv.org/abs/2409.06957) |
| 477          | RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval                                   | [Link](https://arxiv.org/abs/2409.10516) |
| 478          | Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement                               | [Link](https://arxiv.org/abs/2409.12122) |
| 479          | Qwen2.5-Coder Technical Report                                                                                     | [Link](https://arxiv.org/abs/2409.12186) |
| 480          | Instruction Following without Instruction Tuning                                                                   | [Link](https://arxiv.org/abs/2409.14254) |
| 481          | Is Preference Alignment Always the Best Option to Enhance LLM-Based Translation? An Empirical Analysis            | [Link](https://arxiv.org/abs/2409.20059) |
| 482          | The Perfect Blend: Redefining RLHF with Mixture of Judges                                                          | [Link](https://arxiv.org/abs/2409.20370) |
| 483          | Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations                                     | [Link](https://arxiv.org/abs/2411.00640) |
| 484          | Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation              | [Link](https://arxiv.org/abs/2411.00412) |
| 485          | Multi-expert Prompting Improves Reliability, Safety, and Usefulness of Large Language Models                        | [Link](https://arxiv.org/abs/2411.00492) |
| 486          | Sample-Efficient Alignment for LLMs                                                                                 | [Link](https://arxiv.org/abs/2411.01493) |
| 487          | A Comprehensive Survey of Small Language Models in the Era of Large Language Models                                 | [Link](https://arxiv.org/abs/2411.03350) |
| 488          | ‚ÄúGive Me BF16 or Give Me Death‚Äù? Accuracy-Performance Trade-Offs in LLM Quantization                                | [Link](https://arxiv.org/abs/2411.02355) |
| 489          | Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation                                   | [Link](https://arxiv.org/abs/2411.02462) |
| 490          | HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems                             | [Link](https://arxiv.org/abs/2411.02959) |
| 491          | Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination                             | [Link](https://arxiv.org/abs/2411.03823) |
| 492          | Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding                   | [Link](https://arxiv.org/abs/2411.04282) |
| 493          | Number Cookbook: Number Understanding of Language Models and How to Improve It                                     | [Link](https://arxiv.org/abs/2411.03766) |
| 494          | Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models                      | [Link](https://arxiv.org/abs/2411.04996) |
| 495          | BitNet a4.8: 4-bit Activations for 1-bit LLMs                                                                       | [Link](https://arxiv.org/abs/2411.04965) |
| 496          | Scaling Laws for Precision                                                                                        | [Link](https://arxiv.org/abs/2411.04330) |
| 497          | Energy Efficient Protein Language Models                                                                           | [Link](https://arxiv.org/abs/2411.05966) |
| 498          | Balancing Pipeline Parallelism with Vocabulary Parallelism                                                        | [Link](https://arxiv.org/abs/2411.05288) |
| 499          | Toward Optimal Search and Retrieval for RAG                                                                        | [Link](https://arxiv.org/abs/2411.07396) |
| 500          | Large Language Models Can Self-Improve in Long-context Reasoning                                                  | [Link](https://arxiv.org/abs/2411.08147) |
|--------------|-----------------------------------------------------------------------------------------------------------------------|-------------------------------------|
| 501          | Stronger Models are NOT Stronger Teachers for Instruction Tuning                                                  | [Link](https://arxiv.org/abs/2411.07133) |
| 502          | Direct Preference Optimization Using Sparse Feature-Level Constraints                                             | [Link](https://arxiv.org/abs/2411.07618) |
| 503          | Cut Your Losses in Large-Vocabulary Language Models                                                               | [Link](https://arxiv.org/abs/2411.09009) |
| 504          | Does Prompt Formatting Have Any Impact on LLM Performance?                                                        | [Link](https://arxiv.org/abs/2411.10541) |
| 505          | SymDPO: Boosting In-Context Learning of Large Multimodal Models                                                   | [Link](https://arxiv.org/abs/2411.11909) |
| 506          | SageAttention2 Technical Report                                                                                   | [Link](https://arxiv.org/abs/2411.10958) |
| 507          | Bi-Mamba: Towards Accurate 1-Bit State Space Models                                                               | [Link](https://arxiv.org/abs/2411.11843) |
| 508          | RedPajama: An Open Dataset for Training Large Language Models                                                     | [Link](https://arxiv.org/abs/2411.12372) |
| 509          | Hymba: A Hybrid-head Architecture for Small Language Models                                                      | [Link](https://arxiv.org/abs/2411.13676) |
| 510          | Loss-to-Loss Prediction: Scaling Laws for All Datasets                                                            | [Link](https://arxiv.org/abs/2411.12925) |
| 511          | When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training                                 | [Link](https://arxiv.org/abs/2411.13476) |
| 512          | Multimodal Autoregressive Pre-training of Large Vision Encoders                                                  | [Link](https://arxiv.org/abs/2411.14402) |
| 513          | Natural Language Reinforcement Learning                                                                          | [Link](https://arxiv.org/abs/2411.14251) |
| 514          | Large Multi-modal Models Can Interpret Features in Large Multi-modal Models                                       | [Link](https://arxiv.org/abs/2411.14982) |
| 515          | T√úLU 3: Pushing Frontiers in Open Language Model Post-Training                                                   | [Link](https://arxiv.org/abs/2411.15124) |
| 516          | MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs                                              | [Link](https://arxiv.org/abs/2411.15296) |
| 517          | LLMs Do Not Think Step-by-step In Implicit Reasoning                                                             | [Link](https://arxiv.org/abs/2411.15862) |
| 518          | O1 Replication Journey ‚Äì Part 2                                                                                  | [Link](https://arxiv.org/abs/2411.16489) |
| 519          | Star Attention: Efficient LLM Inference over Long Sequences                                                      | [Link](https://arxiv.org/abs/2411.17116) |
| 520          | Low-Bit Quantization Favors Undertrained LLMs                                                                    | [Link](https://arxiv.org/abs/2411.17691) |
| 521          | Rethinking Token Reduction in MLLMs                                                                              | [Link](https://arxiv.org/abs/2411.17686) |
| 522          | Reverse Thinking Makes LLMs Stronger Reasoners                                                                   | [Link](https://arxiv.org/abs/2411.19865) |
| 523          | Critical Tokens Matter                                                                                          | [Link](https://arxiv.org/abs/2411.19943) |

---

üìå *Explore cutting-edge research ‚ú® and stay updated!* üß†üåü
